
标准线性回归中通过最小二乘法进行参数估计，

$$
\begin{aligned}
\hat{\boldsymbol{w}}^{*}&=\underset{\hat{\boldsymbol{w}}}{\arg \min } \sum_{i=1}^{m}\left(y_{i}-\hat{\boldsymbol{w}}^\mathrm{T}\hat{\boldsymbol{x}}_{i}\right)^{2} 
\end{aligned}
\tag{1}
$$

当$\mathbf{X}^\mathrm{T}\mathbf{X}$为可逆矩阵时，令$\cfrac{\partial E_{\hat{\boldsymbol w}}}{\partial \hat{\boldsymbol w}}=0$，可得$\hat{\boldsymbol w}$最优解的闭式解，如下：

$$
\hat{\boldsymbol{w}}^{*}=(\mathbf{X}^\mathrm{T}\mathbf{X})^{-1}\mathbf{X}^\mathrm{T}y
\tag{2}
$$

然而，现实任务中$X^TX$往往不是满秩矩阵。为得到有效解，岭回归在矩阵$X^TX$对角线元素上加一个小的常数值$\lambda$，

$$
\hat{\boldsymbol{w}}^{*}_{ridge}=(\mathbf{X}^\mathrm{T}\mathbf{X}+\lambda I_n)^{-1}\mathbf{X}^\mathrm{T}y
\tag{3}
$$

其中，$I_n$是单位矩阵；$\lambda$为岭系数。

相应地，代价函数在原有残差平方和的基础上添加了对系数值的惩罚，参数估计的目标改为:

$$
\begin{aligned}
\hat{\boldsymbol{w}}^{*}_{ridge}&=\underset{\hat{\boldsymbol{w}}}{\arg \min } \sum_{i=1}^{m}\left(y_{i}-\hat{\boldsymbol{w}}^\mathrm{T}\hat{\boldsymbol{x}}_{i}\right)^{2}+\lambda \sum_{i=0}^{n} w_i^2 \\
&=\underset{\hat{\boldsymbol{w}}}{\arg \min } ||y-X{\boldsymbol{w}}||+\lambda|| w||

\end{aligned}
\tag{4}
$$

岭回归是一种改良的最小二乘估计法，通过放弃最小二乘法的无偏性，以损失部分信息、降低精度为代价获得回归系数，它是更为符合实际、更可靠的回归方法，对存在离群点的数据的拟合要强于最小二乘法。

不同与线性回归的无偏估计，岭回归的优势在于它的无偏估计，更趋向于将部分系数向0收缩。因此，它可以缓解多重共线问题，以及过拟合问题。但是由于岭回归中并没有将系数收缩到0，而是使得系数整体变小，因此，某些时候模型的解释性会大大降低，也无法从根本上解决多重共线问题。

## 参考资料
1. https://blog.csdn.net/weixin_44225602/article/details/112912067
2. https://blog.csdn.net/weixin_43374551/article/details/83688913
