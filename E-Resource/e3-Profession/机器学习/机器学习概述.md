机器学习是研究关于** “学习算法”** 的学问，致力于研究如何通过计算的手段，利用经验来改善系统自身的性能。==！！！数据决定模型的上限，而算法则是让模型无限逼近上限！！！==

数据—>学习算法—>模型。

## 基本术语

#### **样本**

也称为“示例”，是关于一个事件或对象的描述。任何事物都可以由若干“特征”（或称为“属性”）唯一刻画出来，而向量的各个维度即可用来描述各个特征。

#### **标记**

机器学习的本质就是在学习样本在某个方面的表现是否存在潜在的规律，称该方面的信息为“标记”。

根据标记的取值类型不同，可将机器学习任务分为以下两类：
- 分类(Classification)
- 回归(Regression)

根据是否用到标记，可将机器学习任务分为以下两类：
- 有监督学习()
- 无监督学习()

#### **样本空间**

也称为**“输入空间”或“属性空间”** 。由于样本采用的是标明各个特征取值的“特征向量”来进行表示，表示样本的特征向量所在的空间为样本空间，通常用花式大写的$\mathcal{X}$表示。

#### **标记空间**

标记所在的空间。

#### **数据集**

令集合$D={x_1,x_2,\dots,x_m}$表示包含$m$个样本的数据集，一般同一份数据集中的每个样本都含有相同个数的特征，假设此数据集中的每个样本都含有$d$个特征，则第$i$个样本的数学表示为d维向量：$x_i=(x_{i1};x_{i2};\dots x_{id})$。

#### **模型**

也称为**"学习器"** 。机器学习的一般流程：收集样本，划分为**训练集** 和**测试集** ，选用某个机器学习算法，让其在训练集上进行** “学习”（或称为“训练”）**，然后产出得到**“模型”（或称为“学习器")** ，最后用测试集来测试模型的效果。模型由于未必是真实的规律，所以也叫做**“假设”** 。

#### **泛化**

由于机器学习的目标是根据已知来对未知做出尽可能准确的判断，因此对未知事物判断的准确与否才是衡量一个模型好坏的关键，我们称此为“泛化”能力。

#### **分布**

此处的“分布”是指概率论中的概率分布，通常假设样本空间服从一个未知分布$D$，而我们收集到的每个样本都是独立地从该分布中采样得到，即“独立同分布”。通常收集到的样本越多，越能从样本中反推出 $D$ 的信息，即越接近真相。此假设属于机器学习中的经典假设，在后续学习机器学习算法过程中会经常用到。

## 假设空间和版本空间

**归纳（induction)**是从特殊到一般的“泛化”（generalization）过程，即从具体的事实归结出一般性规律；
**演绎（deduction）** 则是从一般到特殊的勺“特化”（specialization）过程，即从基础原理推演出具体状况.

“从样例中学习”显然是一个归纳的过程，因此亦称“归纳学习”（inductive learning)。归纳学习有狭义与广义之分，广义的归纳学习大体相当于从样例中学习，而狭义的归纳学习则要求从训练数据中学得概念（concept)，因此亦称为“概念学习”或“概念形成”。

数据作为训练集可以有多个**“假设空间”** ，且在不同的假设空间中都有可能学得能够拟合训练集的模型，我们将所有能够拟合训练集的模型构成的集合称为**“版本空间“** 。


## 归纳偏好

机器学习算法在学习过程中对某种类型假设的偏好，称为**“归纳偏好”（inductivebias)** 或简称为** “偏好”** 。不同的机器学习算法有不同的偏好，如何评价模型的好坏呢？著名的“奥卡姆剃刀”原则认为“若有多个假设与观察一致，则选最简单的那个”，但是何为“简单”便见仁见智。而最常用的方法则是**基于模型在测试集上的表现来评判模型之间的优劣**。

#### ** “没有免费的午餐”定理（NoFreeLunchTheorem，简称NFL）**

假设样本空间$\mathcal{X}$和假设空间$H$都是离散的，令$P(h|X,\mathfrak{L}_{a})$代表算法$\mathfrak{L}_{a}$基于训练数据${X}$产生$h$的概率 ，令$f$代表我们希望学习的真实目标函数。$\mathfrak{L}_{a}$的“训练集外误差”，即$\mathfrak{L}_{a}$在训练集之外的所有样本上的误差为：

$$
E_{o t e}\left(\mathfrak{L}_{a} | X, f\right)=\sum_{h} \sum_{\boldsymbol{x} \in \mathcal{X}-X} P(\boldsymbol{x}) \mathbb{I}(h(\boldsymbol{x}) \neq f(\boldsymbol{x})) P\left(h | X, \mathfrak{L}_{a}\right)
\tag{1}
$$

 其中，$\mathbb{I}(\cdot)$为指示函数，若$(\cdot)$为真则为1，否则为0。
 
考虑二分类问题，且真实目标函数可以是任何函数$\mathcal{X}→{0,1}$，函数空间为${0,1}^{\mathcal{X}}$，对所有可能$f$的按均匀分布对误差求和，有：

$$

\begin{aligned}
\sum_{f}E_{ote}(\mathfrak{L}_a\vert X,f) &= \sum_f\sum_h\sum_{\boldsymbol{x}\in\mathcal{X}-X}P(\boldsymbol{x})\mathbb{I}(h(\boldsymbol{x})\neq f(\boldsymbol{x}))P(h\vert X,\mathfrak{L}_a) \\
&=\sum_{\boldsymbol{x}\in\mathcal{X}-X}P(\boldsymbol{x}) \sum_hP(h\vert X,\mathfrak{L}_a)\sum_f\mathbb{I}(h(\boldsymbol{x})\neq f(\boldsymbol{x})) \\
&=\sum_{\boldsymbol{x}\in\mathcal{X}-X}P(\boldsymbol{x}) \sum_hP(h\vert X,\mathfrak{L}_a)\cfrac{1}{2}2^{\vert \mathcal{X} \vert} \\
&=\cfrac{1}{2}2^{\vert \mathcal{X} \vert}\sum_{\boldsymbol{x}\in\mathcal{X}-X}P(\boldsymbol{x}) \sum_hP(h\vert X,\mathfrak{L}_a) \\
&=2^{\vert \mathcal{X} \vert-1}\sum_{\boldsymbol{x}\in\mathcal{X}-X}P(\boldsymbol{x}) \cdot 1\\
\end{aligned}
\tag{2}
$$

由式$(2)$知，总误差与学习算法无关！

**NFL定理有一个重要前提：所有“问题”出现的机会相同、或所有问题同等重要。但实际情形并不是这样.很多时候，我们只关注自已正在试图解决的问题。**