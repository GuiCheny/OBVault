
**集成学习(Ensemble Learning)**通过构建并结合多个学习器来完成学习任务，有时也被称为多分类器系统(Multi-classifier System)、基于委员会的学习(Committee-based Learning）等。

**集成学习结构**：先产生一组**个体学习器(individuallearner)**，再用某种策略将它们结合起来。个体学习器通常由一个现有的学习算法从训练数据产生，例如C4.5决策树算法、BP神经网络算法等。

![[集成学习示意图.png#pic_center|400]]

- **同质集成(homogeneous ensemble)**：集成中只包含同种类型的个体学习器。
- **基学习器(base learner)**：同质集成中的个体学习器。
- **基学习算法(（baselearning algorithm)**：基学习器相应的学习算法。
- **异质集成(heterogenous ensemble)**: 集成包含不同类型的个体学习器。
- **组件学习器(（component learner))**:异质集成中的个体学习器。

==集成学习*通过将多个学习器进行结合，常可获得比单一学习器显著优越的泛化性能==，这对**弱学习器(weak learner)** 尤为明显。因此集成学习的很多理精度论研究都是针对弱学习器进行的，而基学习器有时也被直接称为弱学习器。
但需注意的是，虽然从论上来说使用弱学习器集成足以获得好的性能，但在实践中出于种种考虑，例如希望使用较少的个体学习器，或是重用关于常见学习器的一些经验等，人们往往会使用比较强的学习器。==要获得好的集成，个体学习器应“好而不同”，即个体学习器要有一定的“准确性”，即学习器不能太坏，并且要有“多样性”（diversity)，即学习器间具有差异。==

考虑二分类问题$y\in \{ -1, +1\}$和真实函数$f$，假定基分类器的错误率为$\epsilon$，即对每个分类器$h_i$有：

$$
P\left(h_{i}(\boldsymbol{x}) \neq f(\boldsymbol{x})\right)=\epsilon
\tag{1}
$$



**[解析]：**$h_{i}(\boldsymbol{x})$是编号为$i$的基分类器给$x$的预测标记，$f(\boldsymbol{x})$是$x$的真实标记，它们之间不一致的概率记为$\epsilon$。

假设集成通过简单投票法结合$T$个基分类器，若有超过半数的基分类器正确，则集成分类就正确：

$$
H(\boldsymbol{x})=\operatorname{sign}\left(\sum_{i=1}^{T} h_{i}(\boldsymbol{x})\right)
$$

**[解析]：**$h_i(\boldsymbol{x})$当把$\boldsymbol{x}$分成1时，$h_i(\boldsymbol{x})=1$，否则$h_i(\boldsymbol{x})=-1$。各个基分类器$h_i$的分类结果求和之后数字的正、负或0，代表投票法产生的结果，即“少数服从多数”，符号函数$\operatorname{sign}$，将正数变成1，负数变成-1，0仍然是0，所以$H(\boldsymbol{x})$是由投票法产生的分类结果。




