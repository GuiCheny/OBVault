## 定义

给定数据集$D=\{(x_1,y_1),(x_2,y_2),...,(x_m,y_m)\}$,其中$\boldsymbol{x_{i1}}=\{x_i;x_{i2};...;x_{id})\}$，$y_i\in\mathbb{R}$

线性回归(Linear regression)试图学得一个线性模型以尽可能准确地预测实值输出标记，即
$$
\begin{align} \large f(x_i) &= wx_i+b \simeq y_i \end{align} \tag{1}
$$

## 均方误差

**均方误差**亦称**平方损失(Square loss)**,对应欧式距离(Euclidean distance)，具有非常好的几何意义。

### 计算公式

**公式如下：**

$$
\begin{align} \large (w^*, b^*) &= \arg \min_{(w, b)} \sum_{i=1}^m \left( f(x_i) - y_i \right)^2 \\ &= \arg \min_{(w, b)} \sum_{i=1}^m \left( y_i - w x_i - b \right)^2 \end{align} \tag{2}
$$

## 最小二乘法

基于均方误差最小化来进行模型求解的方法称为**最小二乘法(least square method)**。线性回归中，最小二乘法就是试图找到一条直线，使所有样本到直线上的欧氏距离之和最小。

### 参数估计

求解$w$和$b$使$E_{(w,b)} = \sum_{i=1}^m \left( y_i - w x_i - b \right)^2$最小化的过程,称为线性回归和模型的**最小二乘“参数估计”（parameter estimation)**。

$E_{(w,b)}$对$w$和$b$分别求导可得，

$$
\cfrac{\partial E_{(w, b)}}{\partial w}=2\left(w \sum_{i=1}^{m} x_{i}^{2}-\sum_{i=1}^{m}\left(y_{i}-b\right) x_{i}\right)
\tag{3}
$$

$$
\cfrac{\partial E_{(w, b)}}{\partial b}=2\left(m b-\sum_{i=1}^{m}\left(y_{i}-w x_{i}\right)\right)
\tag{4}
$$

[公式(3)推导]：已知$E_{(w, b)}=\sum\limits_{i=1}^{m}\left(y_{i}-w x_{i}-b\right)^{2}$，所以

$$
\begin{aligned}
\cfrac{\partial E_{(w, b)}}{\partial w}&=\cfrac{\partial}{\partial w} \left[\sum_{i=1}^{m}\left(y_{i}-w x_{i}-b\right)^{2}\right] \\
&= \sum_{i=1}^{m}\cfrac{\partial}{\partial w} \left[\left(y_{i}-w x_{i}-b\right)^{2}\right] \\
&= \sum_{i=1}^{m}\left[2\cdot\left(y_{i}-w x_{i}-b\right)\cdot (-x_i)\right] \\
&= \sum_{i=1}^{m}\left[2\cdot\left(w x_{i}^2-y_i x_i +bx_i\right)\right] \\
&= 2\cdot\left(w\sum_{i=1}^{m} x_{i}^2-\sum_{i=1}^{m}y_i x_i +b\sum_{i=1}^{m}x_i\right) \\
&=2\left(w \sum_{i=1}^{m} x_{i}^{2}-\sum_{i=1}^{m}\left(y_{i}-b\right) x_{i}\right)
\end{aligned}
$$

[公式(4)推导]：已知$E_{(w, b)}=\sum\limits_{i=1}^{m}\left(y_{i}-w x_{i}-b\right)^{2}$，所以

$$
\begin{aligned}
\cfrac{\partial E_{(w, b)}}{\partial b}&=\cfrac{\partial}{\partial b} \left[\sum_{i=1}^{m}\left(y_{i}-w x_{i}-b\right)^{2}\right] \\
&=\sum_{i=1}^{m}\cfrac{\partial}{\partial b} \left[\left(y_{i}-w x_{i}-b\right)^{2}\right] \\
&=\sum_{i=1}^{m}\left[2\cdot\left(y_{i}-w x_{i}-b\right)\cdot (-1)\right] \\
&=\sum_{i=1}^{m}\left[2\cdot\left(b-y_{i}+w x_{i}\right)\right] \\
&=2\cdot\left[\sum_{i=1}^{m}b-\sum_{i=1}^{m}y_{i}+\sum_{i=1}^{m}w x_{i}\right] \\
&=2\left(m b-\sum_{i=1}^{m}\left(y_{i}-w x_{i}\right)\right)
\end{aligned}
$$

令式$(3)$和$(4)$等于，可得到$w$和$b$最优解的`闭式(closed-form)解`。
$$
w=\cfrac{\sum_{i=1}^{m}y_i(x_i-\bar{x})}{\sum_{i=1}^{m}x_i^2-\cfrac{1}{m}(\sum_{i=1}^{m}x_i)^2}
\tag{5}
$$



[公式$(5)$推导]：令公式(3)等于0


$$
0 = w\sum_{i=1}^{m}x_i^2-\sum_{i=1}^{m}(y_i-b)x_i
$$

$$
w\sum_{i=1}^{m}x_i^2 = \sum_{i=1}^{m}y_ix_i-\sum_{i=1}^{m}bx_i
$$


令式$(4)$等于0，可得$b=\cfrac{1}{m}\sum_{i=1}^{m}(y_i-wx_i)$，又因为$\cfrac{1}{m}\sum_{i=1}^{m}y_i=\bar{y}$，$\cfrac{1}{m}\sum_{i=1}^{m}x_i=\bar{x}$，则$b=\bar{y}-w\bar{x}$，代入上式可得

  
$$

\begin{aligned}  

w\sum_{i=1}^{m}x_i^2 & = \sum_{i=1}^{m}y_ix_i-\sum_{i=1}^{m}(\bar{y}-w\bar{x})x_i \\

w\sum_{i=1}^{m}x_i^2 & = \sum_{i=1}^{m}y_ix_i-\bar{y}\sum_{i=1}^{m}x_i+w\bar{x}\sum_{i=1}^{m}x_i \\

w(\sum_{i=1}^{m}x_i^2-\bar{x}\sum_{i=1}^{m}x_i) & = \sum_{i=1}^{m}y_ix_i-\bar{y}\sum_{i=1}^{m}x_i \\

w & = \cfrac{\sum_{i=1}^{m}y_ix_i-\bar{y}\sum_{i=1}^{m}x_i}{\sum_{i=1}^{m}x_i^2-\bar{x}\sum_{i=1}^{m}x_i}

\end{aligned}

$$

  

由于$\bar{y}\sum_{i=1}^{m}x_i=\cfrac{1}{m}\sum_{i=1}^{m}y_i\sum_{i=1}^{m}x_i=\bar{x}\sum_{i=1}^{m}y_i$，$\bar{x}\sum_{i=1}^{m}x_i=\cfrac{1}{m}\sum_{i=1}^{m}x_i\sum_{i=1}^{m}x_i=\cfrac{1}{m}(\sum_{i=1}^{m}x_i)^2$，代入上式即可得公式(3.7)

  

$$

w=\cfrac{\sum_{i=1}^{m}y_i(x_i-\bar{x})}{\sum_{i=1}^{m}x_i^2-\cfrac{1}{m}(\sum_{i=1}^{m}x_i)^2}

$$


